{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 6 variations of learning rate to be tested and each variation will be evaluated using 10-fold cross validation, meaning that there is a total of 6Ã—10 or 60 XGBoost models to be trained and evaluated.\n",
    "Note that we have fixed the number of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.483013 using {'learning_rate': 0.1}\n",
      "-0.689650 (0.000242) with: {'learning_rate': 0.0001}\n",
      "-0.661274 (0.001954) with: {'learning_rate': 0.001}\n",
      "-0.530747 (0.022961) with: {'learning_rate': 0.01}\n",
      "-0.483013 (0.060755) with: {'learning_rate': 0.1}\n",
      "-0.515440 (0.068974) with: {'learning_rate': 0.2}\n",
      "-0.557315 (0.081738) with: {'learning_rate': 0.3}\n"
     ]
    }
   ],
   "source": [
    "# Tune learning_rate\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "# load data\n",
    "dataset = loadtxt('pima-indians-diabetes.csv', delimiter=\",\")\n",
    "# split data into X and y\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# grid search\n",
    "model = XGBClassifier()\n",
    "learning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "param_grid = dict(learning_rate=learning_rate)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\n",
    "grid_result = grid_search.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot\n",
    "pyplot.errorbar(learning_rate, means, yerr=stds)\n",
    "pyplot.title(\"XGBoost learning_rate vs Log Loss\")\n",
    "pyplot.xlabel('learning_rate')\n",
    "pyplot.ylabel('Log Loss')\n",
    "pyplot.savefig('learning_rate.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Learning Rate and the Number of Trees in XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 5 variations of n_estimators and 4 variations of learning_rate. Each combination will be evaluated using 10-fold cross validation, so that is a total of 4x5x10 or 200 XGBoost models that must be trained and evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.476375 using {'n_estimators': 500, 'learning_rate': 0.01}\n",
      "-0.689650 (0.000242) with: {'n_estimators': 100, 'learning_rate': 0.0001}\n",
      "-0.686232 (0.000472) with: {'n_estimators': 200, 'learning_rate': 0.0001}\n",
      "-0.682878 (0.000678) with: {'n_estimators': 300, 'learning_rate': 0.0001}\n",
      "-0.679574 (0.000875) with: {'n_estimators': 400, 'learning_rate': 0.0001}\n",
      "-0.676337 (0.001069) with: {'n_estimators': 500, 'learning_rate': 0.0001}\n",
      "-0.661274 (0.001954) with: {'n_estimators': 100, 'learning_rate': 0.001}\n",
      "-0.634948 (0.003900) with: {'n_estimators': 200, 'learning_rate': 0.001}\n",
      "-0.613073 (0.006087) with: {'n_estimators': 300, 'learning_rate': 0.001}\n",
      "-0.594517 (0.008463) with: {'n_estimators': 400, 'learning_rate': 0.001}\n",
      "-0.578893 (0.010938) with: {'n_estimators': 500, 'learning_rate': 0.001}\n",
      "-0.530747 (0.022961) with: {'n_estimators': 100, 'learning_rate': 0.01}\n",
      "-0.492309 (0.036248) with: {'n_estimators': 200, 'learning_rate': 0.01}\n",
      "-0.481079 (0.042922) with: {'n_estimators': 300, 'learning_rate': 0.01}\n",
      "-0.477315 (0.049494) with: {'n_estimators': 400, 'learning_rate': 0.01}\n",
      "-0.476375 (0.053288) with: {'n_estimators': 500, 'learning_rate': 0.01}\n"
     ]
    }
   ],
   "source": [
    "n_estimators = [100, 200, 300, 400, 500]\n",
    "learning_rate = [0.0001, 0.001, 0.01]\n",
    "param_grid = dict(learning_rate=learning_rate, n_estimators=n_estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\n",
    "grid_result = grid_search.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "# plot results\n",
    "scores = np.array(means).reshape(len(learning_rate), len(n_estimators))\n",
    "for i, value in enumerate(learning_rate):\n",
    "    pyplot.plot(n_estimators, scores[i], label='learning_rate: ' + str(value))\n",
    "pyplot.legend()\n",
    "pyplot.xlabel('n_estimators')\n",
    "pyplot.ylabel('Log Loss')\n",
    "pyplot.savefig('n_estimators_vs_learning_rate.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "References:\n",
    "----------------------------\n",
    "\n",
    "https://machinelearningmastery.com/xgboost-python-mini-course/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
